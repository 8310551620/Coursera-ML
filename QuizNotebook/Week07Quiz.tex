
\documentclass[11pt]{article} % use larger type; default would be 10pt
\usepackage{framed}
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{framed}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\begin{document}
	
\section{Week 7 Unsupervised Learning}
\subsection{K-means Clustering}


\begin{itemize}
	\item In this this exercise, you will implement the K-means algorithm and use it for image compression.
	
	\item You will first start on an example 2D dataset that will help you gain an intuition of
	how the K-means algorithm works. After that, you wil use the K-means algorithm for image
	compression by reducing the number of colors that occur in an image to only those that are most common in that image.
	
	\item You will be using ex7.m for this part of the exercise.
	
\end{itemize}

\subsection{Implementing K-means}
\begin{itemize}
	\item The K-means algorithm is a method to automatically cluster similar data examples together.
	
	\item The intuition behind K-means is an iterative procedure that starts by guessing the initial centroids, and then refines this guess by repeatedly assigning examples to their closest centroids and then recomputing the centroids based on the assignments.
	
	\item The inner-loop of the algorithm repeatedly carries out two steps: 
	\begin{itemize}
		\item[(i)] Assigning each training example to its closest centroid 
		\item[(ii)] Recomputing the mean of each centroid using the points assigned to it. 
	\end{itemize}
	
	
	\item The K-means algorithm will always converge to some final set of means for the centroids.
	Note that the converged solution may not always be ideal and depends on the initial setting of the centroids. 
	
	\item Therefore, in practice the K-means algorithm is usually run a few times with different random initializations. 
	
	\item One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).
	
	\item Random initialization
	The initial assignments of centroids for the example dataset in \texttt{ex7.m} were designed so that you will see the same gure as in Figure 1. 
	
	\item In practice, a good strategy for initializing the centroids is to select random examples from the training set.
\end{itemize}
%=================================================================%
\subsection*{Question 1. }
For which of the following tasks might K-means clustering be a suitable algorithm? Select all that apply.


%=================================================================%
\subsection*{Question 2.} 
Suppose we have three cluster centroids $\mu_1$=$[1 2]^T$, $\mu_2$=$[-3 0]^T$ and $\mu_3$=$[4 2]^T$. 
Furthermore, we have a training example x(i)=$[3 1]^T$. After a cluster assignment step, what will $c^{(i)}$ be?


%=================================================================%
\subsection*{Question 3.}
K-means is an iterative algorithm, and two of the following steps are repeatedly carried out in its inner-loop. Which two?


%=================================================================%
\subsection*{Question 4. }
Suppose you have an unlabeled dataset $\{x(1),\ldots,x(m)\}$. You run K-means with 50 different random
initializations, and obtain 50 different clusterings of the data. 
What is the recommended way for choosing which one of these 50 clusterings to use?

%=================================================================%
\subsection*{Question 5. }
Which of the following statements are true? Select all that apply.



%====================================================================%
\newpage
\section*{Anomaly Detection}
\begin{itemize}
	\item Suppose you are developing an anomaly detection system to catch manufacturing defects in airplane engines. 
	\item Your model uses
	{ 
		\Large
		\[p(x)= \Pi ^{n}_{j=1} p(x_j;\mu_j,\sigma^2_j)\] 
	}
	\item You have two features $x_1$ = \textit{\textbf{vibration intensity}}, and $x_2$ = \textit{\textbf{heat generated}}. 
	\item Both $x_1$ and $x_2$ take on values between 0 and 1 (and are strictly greater than 0), and for most "normal" engines you expect that $x_2 \approx x_2$. 
	\item One of the suspected anomalies is that a flawed engine may vibrate very intensely even without generating much heat (large $x_1$, small $x_2$), 
	even though the particular values of $x_1$ and $x_2$ may not fall outside their typical ranges of values. 
	\item What additional feature $x_3$ should you create to capture these types of anomalies:
\end{itemize}
\textbf{Solution Options}
\begin{itemize}
	\item $x_3=x_1+x_2$	This could take on large or small values for both normal and anomalous examples, so it is not a good feature.
\end{itemize}

\section{Week 9 Quiz. Anomaly Detection}

%--------------------------------------------------------------------%
\subsection*{Question 1. }
For which of the following problems would \textbf{anomaly detection} be a suitable algorithm?

\begin{itemize}
	\item From a large set of hospital patient records, predict which patients have a particular disease (say, the flu).
	\item From a large set of primary care patient records, identify individuals who might have unusual health conditions.
	\item CORRECT
	Given data from credit card transactions, classify each transaction according to type of purchase (for example: food, transportation, clothing).
	\item 
	In a computer chip fabrication plant, identify microchips that might be defective.
\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 2. Variant A}
Suppose you have trained an anomaly detection system that flags anomalies when p(x) is less than $\epsilon$, and you find on the cross-validation set that it has 
too many false negatives (failing to flag a lot of anomalies). What should you do?


\begin{itemize}
	\item Increase $\epsilon$ [CORRECT]
	\item Decrease $\epsilon$
\end{itemize}

\subsection*{Question 2. Variant B}
Suppose you have trained an anomaly detection system for fraud detection, and your system that flags 
anomalies when p(x) is less than $\epsilon$, and you find on the 
cross-validation set that it mis-flagging far too many good transactions as fradulent. What should you do?

\begin{itemize}
	\item Increase $\epsilon$ 
	\item Decrease $\epsilon$ [CORRECT]
\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 3. }
Suppose you are developing an anomaly detection system to catch manufacturing defects in airplane engines. You model uses

% \[p(x)=‚àènj=1p(xj;\muj,\sigma^2_j).\]

You have two features $x_1$ = vibration intensity, and $x_2$ = heat generated. 

Both x1 and x2 take on values between 0 and 1 (and are strictly greater than 0), and for most "normal" engines you expect that $x_1 \approx x_2$. 

One of the suspected anomalies is that a flawed engine may vibrate very intensely even without generating much heat (large x1, small x2), even though the particular values of x1 and x2 may not fall outside their typical ranges of values. What additional feature x3 should you create to capture these types of anomalies:

\begin{itemize}
	\item x3=x1/x2 [CORRECT]
	
	\item x3=x21 $times$ x2
	
	\item x3=x1 $times$ x2
	
	\item  x3=x1+x2
	
\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 4. }
Which of the following are true? Check all that apply.

\begin{itemize}
	\item When evaluating an anomaly detection algorithm on the cross validation set (containing some positive and some negative examples), classification accuracy is usually a good evaluation metric to use.
	\item  In anomaly detection, we fit a model p(x) to a set of negative (y=0) examples, without using any positive examples we may have collected of previously observed anomalies.
	\item \textbf{CORRECT} When developing an anomaly detection system, it is often useful to select an appropriate numerical performance metric to evaluate the effectiveness of the learning algorithm.
	\item In a typical anomaly detection setting, we have a large number of anomalous examples, and a relatively small number of normal/non-anomalous examples.
\end{itemize}
%--------------------------------------------------------------------%
\subsection*{Question 5. }
You have a 1-D dataset $\{x(1),\ldots,x(m)\}$ and you want to detect outliers in the dataset. You first plot the dataset and it looks like this:


Suppose you fit the gaussian distribution parameters $\mu_1$ and $\sigma^2_1$ to this dataset. Which of the following values for $\mu_1$ and $\sigma^2_1$ might you get?
\begin{itemize}
	\item $\mu_1$=-3,$\sigma^2_1$=4
	\item 
	$\mu_1$=-6,$\sigma^2_1$=4
	\item
	$\mu_1$=-3,$\sigma^2_1$=2
	\item 
	$\mu_1$=-6,$\sigma^2_1$=2
\end{itemize}



%------------------------------------------------%
% ML Week 9

\section{Week 9 Quiz. Recommender Systems}

Information Filtering System that attempts to recommend information items likely
to be of interest to a user.

\textbf{Commonly used algorithms}
\begin{itemize}
	\item $k-$means clustering
	\item Pearson's Rho
	\item Collaborative Filtering
\end{itemize}

Collaborative Filtering is the process of filtering for information or patterns using collaboration among multiple
agents.

Applications: online news aggregation or similar items of clothings

best approached by other methods - prediction

Collaborative Filtering Gradient
%NOT FINISHED

\[ \frac{\partial J}{\partial X^{(i)}_k}  = \sum [  ] \theta^{(j)}_k \]
\[ \frac{\partial J}{\partial \theta^{(i)}_k}  = \sum [  ] X^{(j)}_k \]

No regularization applied

%--------------------------------------%
Anomaly Detection
Gaussian Distribution
Estimate Gaussian Distribution

For $n$ feastures of $X$ , compute the mean and variance for each feature

%-----------%
Selecting Threshold of $\epsilon$.

Implement an algorithm to select the threshold $\epsilon$ using an $F_i$ score on a 
cross validation set.

$P(X) < \epsilon$ is considered to be an anomaly.

\subsection{$F_1$ Score}


\end{document}


%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Question 1

Suppose that you have trained a logistic regression classifier, and it outputs on a new examplex a prediction h\theta(x) = 0.7.
This means (check all that apply): 
 Our estimate for$ \Pr(y|=1|x;\theta)$ is 0.7.
 Our estimate for$ \Pr(y|=0|x;\theta)$ is 0.3.
 Our estimate for$ \Pr(y|=1|x;\theta)$ is 0.3.
 Our estimate for$ \Pr(y|=0|x;\theta)$ is 0.7.
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
Solution
 
Our estimate for $ \Pr(y|=1|x;\theta)$ is 0.7.  T h\theta(x)is precisely$ \Pr(y|=1|x;\theta)$ , so each is 0.7.
Our estimate for $ \Pr(y|=0|x;\theta)$ is 0.3.  T Since we must have $ \Pr(y|=0|x;\theta)$ = 1−$ \Pr(y|=1|x;\theta)$ , the former is 1−0.7=0.3 .
Our estimate for $ \Pr(y|=1|x;\theta)$ is 0.3.  F h\theta(x) gives $ \Pr(y|=1|x;\theta)$ , not 1−$ \Pr(y|=1|x;\theta)$ . 
Our estimate for $ \Pr(y|=0|x;\theta)$ is 0.7.  F h\theta(x) is $ \Pr(y|=1|x;\theta)$ , not $ \Pr(y|=0|x;\theta)$ 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Question 3
 
Suppose you have the following training set, and fit a logistic regression classifier h\theta(x)=g(\theta0+\theta1x1+\theta2x2) .
  
Which of the following are true? Check all that apply. 
 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
 Adding polynomial features (e.g., instead using h(x)=g(0+1x1+2x2+3x12+4x1x2+5x22) ) could increase how well we can fit the training data.  
The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge.
At the optimal value of\theta (e.g., found by fminunc), we will have $ J(\theta) \geq $0 . 
 Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data.
solution
 
1)
Adding polynomial features (e.g., instead using h(x)=g(0+1x1+2x2+3x12+4x1x2+5x22) ) could increase how well we can fit the training data.  
TRUE
Adding new features can only improve the fit on the training set: since setting\theta3=\theta4=\theta5=0 makes the hypothesis the same as the original one, gradient descent will use those features (by making the corresponding\thetaj non-zero) only if doing so improves the training set fit. 
 
2)
The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge. 
FALSE
While it is true they cannot be separated, gradient descent will still converge to the optimal fit. Some examples will remain misclassified at the optimum. 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
3)
At the optimal value of\theta (e.g., found by fminunc), we will have $ J(\theta)$$≥$0 . 
TRUE
The cost function $ J(\theta)$ is always non-negative for logistic regression. 
 
4)
Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data. 
FALSE
While it is true they cannot be separated, logistic regression will outperform linear regression since its cost function focuses on classification, not prediction. 
\end{frame}
%======================================================%
\begin{frame}
\frametitle{Machine Learning} 
\large 

Question 5
Which of the following statements are true? Check all that apply.
 For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).
 Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do one-vs-all classification).
 The sigmoid function g(z) is never greater than one (>1). g(z)=11+e-z
 The one-vs-all technique allows you to use logistic regression for problems in which eachy(i) comes from a fixed, discrete set of values.
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Solutions
1)
For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).  0.00
The cost function for logistic regression is convex, so gradient descent will always converge to the global minimum. We still might use a more advanded optimization algorithm since they can be faster and don't require you to select a learning rate. 
2)
Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do one-vs-all classification).  
We need to train three classifiers if there are three classes; each one treats one of the three classes as the y=1 examples and the rest as the y=0 examples. 
3)
The sigmoid function g(z) is never greater than one (>1). g(z)=11+e-z  
 
The denomiator ranges from  to1 asz grows, so the result is always in(0,1) . 
4)
The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values. 
If each y(i) is one of k different values, we can give a label to each y(i)∈{1,2,…,k} and use one-vs-all as described in the lecture. 
\end{frame}



\section{}

X1= c(89,72,94,69)
X2= c(7921,5184,8836,4761)
Y = c(96,74,87,78)



The mean of x2 is 6675.5 and the range is 8836-4761=4075 So x(1)1 is 4761-6675.54075=-0.47.

%---------------------------------------------------------------------------%
\subsection*{Question 2}
You run gradient descent for 15 iterations with $\alpha$=0.3 and compute J($\theta$) after each iteration. 
You find that the value of J($\theta$) decreases quickly then levels off. Based on this, which of the following conclusions seems most plausible?
Your Answer		Score	Explanationn
\begin{itemize}
\item Rather than use the current value of $\alpha$, it'd be more promising to try a larger value of $\alpha$ (say $\alpha$=1.0).
\\ Incorrect	0.00	 A larger value for $\alpha$ will make it more likely that J($\theta$) diverges.
\item Rather than use the current value of $\alpha$, it'd be more promising to try a smaller value of $\alpha$ (say $\alpha$=0.1).			
\item $\alpha$=0.3 is an effective choice of learning rate.			
\end{itemize}
Total		0.00 / 1.00	

 Rather than use the current value of a, it'd be more promising to try a smaller value of $\alpha$ (say $\alpha$=0.1).	Correct	1.00	 
Since the cost function is increasing, we know that gradient descent is diverging, so we need a lower learning rate.
%---------------------------------------------------------------------------%
%---------------------------------------------------------------------------%
\subsection*{Question 3}
Suppose you have m=28 training examples with n=4 features (excluding the additional 
all-ones feature for the intercept term, which you should add). The normal equation is $\theta$=(XTX)-1XTy. 
For the given values of m and n, what are the dimensions of $\theta$, X, and y in this equation?

\begin{itemize}
\item X is 28×4, y is 28×1, $\theta$ is 4×4			
\item X is 28×4, y is 28×1, $\theta$ is4×1			
\item X is 28×5, y is 28×1, $\theta$ is 5×1	\textbf{Correct	1.0}	
\item X is 28×5, y is 28×5, $\theta$ is 5×5
\end{itemize}			
Total		1.00 / 1.00	
Question Explanation

X has m rows and n+1 columns (+1 because of the x0=1 term). y is an m-vector. $\theta$ is an (n+1)-vector.
%---------------------------------------------------------------------------%
%---------------------------------------------------------------------------%
\subsection*{Question 4}
Suppose you have a dataset with m=50 examples and n=200000 features for each example. 
You want to use multivariate linear regression to fit the parameters $\theta$ to our data. 
Should you prefer gradient descent or the normal equation?

\begin{itemize}
\item Gradient descent, since ($X^TX)^{-1}$ will be very slow to compute in the normal equation.	
\\ Correct
		
\item The normal equation, since it provides an efficient way to directly find the solution.			
\item The normal equation, since gradient descent might be unable to find the optimal $\theta$.	
\\Incorrect	0.00	 \\ For an appropriate choice of $\alpha$, gradient descent can always find the optimal $\theta$.
\item Gradient descent, since it will always converge to the optimal $\theta$.			
\end{itemize}	
%---------------------------------------------------------------------------%
%---------------------------------------------------------------------------%
\subsection*{Question 5}
Which of the following are reasons for using feature scaling?

%--------------%
FALSE It prevents the matrix XTX (used in the normal equation) from being non-invertable (singular/degenerate).	
\\Correct	0.25	 XTX can be singular when features are redundant or there are too few examples. Feature scaling does not solve these problems.
%--------------%
FALSE It speeds up gradient descent by making it require fewer iterations to get to a good solution.	
\\Correct	0.25	 Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest.
%--------------%
TRUE It speeds up solving for $\theta$ using the normal equation.	
\\Incorrect	0.00	 The magnitude of the feature values are insignificant in terms of computational cost.
%--------------%
FALSE It is necessary to prevent gradient descent from getting stuck in local optima.	
\\Correct	0.25	 The cost function J($\theta$) for linear regression has no local optima.
%--------------%	
\end{document}
